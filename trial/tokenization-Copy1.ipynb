{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b62662-7d1f-4c64-9a63-9e19a738eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source /home/sanjana/ML_Sem3_Project/chess-nlp-env/bin/activate\n",
    "\n",
    "import chess.pgn  # (Portable Game Notation)\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For a progress bar, useful in case of large files\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# 1) and 2)\n",
    "# Start by activating the virtual environment: source chess-nlp-env/bin/activate\n",
    "\n",
    "pgn_path = \"../data/processed/lichess_db_standard_rated_2014-09.pgn\"\n",
    "output_csv = \"../data/processed/chess_games.csv\"\n",
    "\n",
    "num_games = 10000  # First 5000 games from the dataset\n",
    "min_plies_required = 6\n",
    "\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fout:  # write mode ('w') with newline='' to prevent extra blank rows.\n",
    "    writer = csv.writer(fout)  # writer object is responsible for handling the CSV formatting.\n",
    "    writer.writerow([\"white\", \"white_elo\", \"black\", \"black_elo\", \"result\", \"date\", \"moves\"])\n",
    "\n",
    "    with open(pgn_path, encoding=\"utf-8\") as pgn_file:  # with block ensures the file is automatically closed when done (or on error)\n",
    "        for _ in tqdm(range(num_games)):\n",
    "            game = chess.pgn.read_game(pgn_file)  # reads the next PGN entry from the open file and returns a Game object\n",
    "            if game is None:\n",
    "                break  # end of file\n",
    "    \n",
    "            # 'read_game' reads whole PGN blocks (headers + move text)\n",
    "            try:\n",
    "                # All the features in the data\n",
    "                # game.headers: a dict-like mapping of PGN tag names to values\n",
    "                # .get(key, default): returns value if present, else default '?' prevents KeyError and preserves placeholder if metadata missing.\n",
    "                white = game.headers.get(\"White\", \"?\")\n",
    "                white_elo = game.headers.get(\"WhiteElo\", \"?\")\n",
    "                black = game.headers.get(\"Black\", \"?\")\n",
    "                black_elo = game.headers.get(\"BlackElo\", \"?\")\n",
    "                result = game.headers.get(\"Result\", \"?\")\n",
    "                date = game.headers.get(\"Date\", \"?\")\n",
    "    \n",
    "                board = game.board()  # An actual chess board\n",
    "                moves = []\n",
    "                for move in game.mainline_moves():  # Gives a move object\n",
    "                    try:\n",
    "                        san = board.san(move)  # board.san(move): Python-Chess verifies that the move is legal from the current position.\n",
    "                        moves.append(san)\n",
    "                        board.push(move)  # updates board state\n",
    "                    except Exception:\n",
    "                        # Skip illegal move: illegal move raises an Assertion error\n",
    "                        moves = []\n",
    "                        break\n",
    "    \n",
    "                if not moves or len(moves) < min_plies_required:\n",
    "                    continue  # skip this game and minimum number of plies/tokens for a checkmate is 6: The fool's mate\n",
    "    \n",
    "                writer.writerow([white, white_elo, black, black_elo, result, date, \" \".join(moves)])  # Instead of storing game as a list, saving memory\n",
    "                                                                                      # and writing directly to csv\n",
    "\n",
    "            except Exception as e:\n",
    "                # Skip the whole game if anything else fails\n",
    "                continue\n",
    "\n",
    "print(f\"\\nFinished processing {num_games} games.\")\n",
    "print(f\"Clean data written to: {output_csv}\")\n",
    "\n",
    "data = pd.read_csv(output_csv)\n",
    "print(f\"Number of valid games: {len(data)}.\\n\")\n",
    "print(data.head())\n",
    "\n",
    "data.info()\n",
    "print(\"\\n---\\n\")  # separator\n",
    "\n",
    "print(\"Data shape:\", data.shape, \"\\n\")\n",
    "\n",
    "# This splits each move by \" \" and creates a list of all moves in a column, then calculates their length and then this new Series\n",
    "# that contains lengths of moves in a game is used for getting Statistical Summary of the number of moves in each game\n",
    "print(data['moves'].apply(lambda s: len(s.split())).describe(), \"\\n\")  # Analyzes the distribution of move lengths\n",
    "\n",
    "# Adding the num_moves column that will have number of moves in each game\n",
    "data[\"moves_list\"] = data[\"moves\"].apply(lambda x: x.split(\" \"))\n",
    "data[\"num_moves\"] = data[\"moves_list\"].apply(len)\n",
    "\n",
    "print(data['result'].value_counts())\n",
    "\n",
    "# Histogram of game lengths\n",
    "\n",
    "data['num_moves'].hist(bins = 30)\n",
    "plt.xlabel(\"Number of Moves\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 3) and 4)\n",
    "\n",
    "# Building a vocab dictionary\n",
    "\n",
    "# each row is a list in data['moves_list']\n",
    "# .explode() expands it so that each list element becomes a separate row and .unique() extracts distinct SAN strings\n",
    "unique_moves = data['moves_list'].explode().unique()\n",
    "print(unique_moves[:20])\n",
    "\n",
    "vocab = {move: idx for idx, move in enumerate(unique_moves, start = 2)}\n",
    "cnt = 0\n",
    "print(\"\\nVocab\")\n",
    "for move, idx in vocab.items():\n",
    "    if cnt < 20:\n",
    "        print(move, \": \", idx)\n",
    "        cnt += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "\n",
    "inv_vocab = {id: mv for mv, id in vocab.items()}  # Inverse vocab for decoding\n",
    "cnt = 0\n",
    "print(\"\\nInverse vocab\")\n",
    "for idx, move in inv_vocab.items():\n",
    "    if cnt < 20:\n",
    "        print(idx, \": \", move)\n",
    "        cnt += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Now, converting every game’s move list to integer IDs\n",
    "data[\"encoded_moves\"] = data['moves_list'].apply(lambda moves: [vocab[m] for m in moves])\n",
    "\n",
    "print(\"\\nEncoded moves list\")\n",
    "data[\"encoded_moves\"].head()\n",
    "\n",
    "len(unique_moves)\n",
    "\n",
    "# 5) \n",
    "# Creating Input_sequence and Target pairs (X, y)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for moves in data[\"moves_list\"]:\n",
    "    for i in range (1, len(moves)):\n",
    "        seq_input = moves[:i]  # All moves till i\n",
    "        seq_output = moves[i]  # Next move\n",
    "        X.append(seq_input)\n",
    "        y.append(seq_output)\n",
    "\n",
    "\n",
    "# Converting input and target to IDs \n",
    "\n",
    "PAD = vocab['<PAD>']\n",
    "UNK = vocab['<UNK>']\n",
    "\n",
    "X_ids = []\n",
    "y_ids = []\n",
    "\n",
    "for seq_input, seq_output in zip(X, y):\n",
    "    input_ids = [vocab.get(m, UNK) for m in seq_input]  # Unknown moves (not in vocab) are replaced with <UNK> \n",
    "    output_ids = vocab.get(seq_output, UNK)\n",
    "\n",
    "    X_ids.append(input_ids)\n",
    "    y_ids.append(output_ids)\n",
    "\n",
    "y_ids[100:110]\n",
    "\n",
    "# 6)\n",
    "# Converting X_ids and y_ids to tensors and padding the sequences in X_ids to bring them to equal length sequences\n",
    "\n",
    "X_tensor = [torch.tensor(seq) for seq in X_ids]  # 2D tensor of size: N x max_move_sequence_length (list of tensors)\n",
    "y_tensor = torch.tensor(y_ids)  # 1D tensor\n",
    "\n",
    "X_padded = pad_sequence(X_tensor, batch_first = True, padding_value = PAD)  # batch_first = True: number of samples\n",
    "\n",
    "\n",
    "\n",
    "# Converting moves to tokens and building vocabulary where vocab = {move: idx} where idx starts at 4 (reserve 0 PAD, 1 UNK, 2 BOS, 3 EOS)\n",
    "# Saving vocabulary in vocab.json and inv_vocab for decoding\n",
    "\n",
    "torch.save({\n",
    "    'vocab': vocab,          # move → ID mapping\n",
    "    'inv_vocab': inv_vocab,  # ID → move mapping\n",
    "    'X_padded': X_padded,\n",
    "    'y_tensor': y_tensor,\n",
    "    'data': data\n",
    "}, \"../data/processed/preprocessed_data.pt\")\n",
    "\n",
    "checkpoint = torch.load(\"../data/processed/preprocessed_data.pt\", weights_only=False)\n",
    "\n",
    "vocab = checkpoint['vocab']\n",
    "inv_vocab = checkpoint['inv_vocab']\n",
    "X_padded = checkpoint['X_padded']\n",
    "y_tensor = checkpoint['y_tensor']\n",
    "data = checkpoint['data']\n",
    "\n",
    "print(\"Loaded preprocessed data successfully.\")\n",
    "\n",
    "# Next steps\n",
    "# Implement and train LSTM baseline; then implement Transformer from scratch.\n",
    "\n",
    "'''\n",
    "1) Split data into training and validation\n",
    "2) Create DataLoader\n",
    "3) Define your LSTM model\n",
    "4) Define Loss and Optimizer\n",
    "5) Train the model\n",
    "6) Validate\n",
    "7) Predict and Decode\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
